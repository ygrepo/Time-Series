\documentclass[12pt]{article}

\usepackage[a4paper,margin=0.5in]{geometry}

\usepackage[square,numbers,sort&compress]{natbib}
%\usepackage[sort&compress]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{times}
\usepackage[psamsfonts]{amssymb}
\usepackage{latexsym}
\usepackage{color}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{amstext}
\usepackage{blkarray}
\usepackage{url}
\usepackage{epsfig}
\usepackage{bm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}
\usepackage{mathtools}


\usepackage{graphicx}
\DeclareMathOperator*{\E}{\rm E}


\newcommand{\bigo}[1]{{\cal O}\left(#1 \right)}
\newcommand{\p}{\mathrm{P}}
\newcommand{\vect}[1]{\bm{#1}} % vectors
\newcommand{\matr}[1]{\bm{#1}} % matrices and tensors

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Covar}{\mathrm{Covar}}
\newcommand{\tr}{^\top}
\begin{document}
\thispagestyle{empty}
\begin{center}

\textbf{DS-GA 3001.001 Special Topics in Data Science: Probabilistic Time Series Analysis\\
Homework 4}
\end{center}

\noindent \textbf{Due date: Nov  22}\\
\noindent YG390\\


\noindent \textbf{Problem 1. (15pt)} Which of these objects are a Gaussian process?
\begin{itemize}
\item linear combination of 2 GPs: $f(x) = a f_1(x) + b f_2(x)$ where $f_i \sim \mathcal{GP}(\mu_i(x); k_i(x,y))$ (independent) and $a,\, b$ are fixed parameters.
The distribution of the sum of the functions due to linearity of Gaussian distributions is simply another $\mathcal{GP}$: $ f(x) \sim  \mathcal{GP}(\mu_1(x) + \mu_2(x); k_1(x,y) + k_2(x,y))$

\item random linear: $ f(x) = a x + w$ where $a \sim \mathcal{N}(0,\sigma_a^2)$, $w\sim \mathcal{N}(0,\sigma_w^2)$.
We consider two fixed $x_1$ and $x_2$ and note that the two random variables $f(x_1)$ and $f(x_2)$ are jointly Gaussian.
\begin{align*}
	\E[f(x_i)]			&=	\E[a x_i + w] \\
					&= 	x_i \E[a] + \E[w] \\
					&=	0 \\
	\Var[f(x)]			&=	\Var[ a x + w] \\
					&= 	x^2 \Var[a] + \Var[w] \\
					&=	x^2 \sigma_a^2 + \sigma_w^2 \\
	\Covar[f(x_1),f(x_2)]	&=	\E (a x_1 + w)(a x_2 + w)] \\
					&=	x_1 x_2 \E[a,a] + \E[w,w]  \\
					&=	x_1 x_2 \sigma_a^2 + \sigma_w^2			
\end{align*}
Let $y_1=f(x_1)$ and $y_2=f(x_2)$, the multivariate gaussian distribution $p(y_1,y_2)$ is:

\[
p(y_1,y_2) = 
	\mathcal{N} \Big(
	\mu = \begin{bmatrix}
		0 \\
		0 \\
	\end{bmatrix}
	,
	\Sigma = \begin{bmatrix}
		x_1^2 \sigma_a^2 + \sigma_w^2 & x_1 x_2 \sigma_a^2 + \sigma_w^2	 \\
		x_1 x_2 \sigma_a^2 + \sigma_w^2	&  x_2^2 \sigma_a^2 + \sigma_w^2 \\
	\end{bmatrix}
	\Big)
\]	

Thus $ f(x) \sim  \mathcal{GP}(\mu(x_1, x_2); \Sigma(x_1,x_2))$.

\item random periodic: $f(x) = a \cos(wx)+ b \sin(wx)$ with $a \sim \mathcal{N}(0,\sigma^2)$, $b \sim \mathcal{N}(0,\sigma^2)$, w fixed parameter.

"From Professor Savin: for all pairs of variables: if nothing about their dependency structure is explicitly specified, they are independent."
Similarly, we have now:
\begin{align*}
	\E[f(x_i)]			&=	\E[a \cos(w x_i) + b \sin(w x_i)] \\
					&= 	\cos(w x_i) \E[a] + \sin(w x_i) \E[b] \\
					&=	0 \\
	\Var[f(x)]			&=	\Var[a \cos(wx)+ b \sin(wx)] \\
					&= 	\cos(wx)^2 \Var[a] + \sin(wx)^2 \Var[w] \\
					&=	\sigma^2  \\					
	\Covar[f(x_1),f(x_2)]	&=	\E[ (a \cos(w x_1) + b  \sin(w x_1)) (a \cos(w x_2) + b  \sin(w x_2))] \\
					&=	\cos(w x_1) \cos(w x_2) \E[a,a] + \sin(w x_1) \sin(w x_2) \E[w,w]  \\
					&=	\cos(w x_1) \cos(w x_2)  \sigma^2 +  \sin(w x_1) \sin(w x_2) \sigma^2  \\
					&= 	\sigma^2 \cos(w (x_1 - x_2))		
\end{align*}

$f(x)$ is a Gaussian process: $ f(x) \sim  \mathcal{GP}(\mu(x_1, x_2); \Sigma(x_1,x_2))$, where $ \mu(x_1, x_2) = \begin{bmatrix}
		0 \\
		0 \\
	\end{bmatrix}$ and $\Sigma =  \begin{bmatrix}
		\sigma^2	& \sigma^2 \cos(w (x_1 - x_2))	 \\
		\sigma^2 \cos(w (x_1 - x_2))	&  \sigma^2 \\
	\end{bmatrix}
	\Big) $

\end{itemize}
If yes, then write down the corresponding mean and covariance functions.\\


\noindent \textbf{Problem 2. (10pt)} 
How would you construct a GP-equivalent of an ARIMA (1,2,1) model?\\

An ARIMA (1,2,1) model $x_t$ could be defined as the combination of a trend and some noise: $x_t = \mu_t + y_t$ where $\mu_t = \beta_0 + \beta_1 t + \beta_2 t^2$ and $y_t$ is an ARMA(1,1).
Differencing such process leads to a stationary process: $\Delta^2 x_t = \beta_2 + \Delta^2 y_t $ which is stationary (constant mean), the corresponding GP-equivalent has for mean function 
$m(.) =  \beta_0 + \beta_1 t + \beta_2 t^2$.
The ARMA $y_t$ process to be sensible as to be causal and suppose that, $y_t = \phi y_{t-1} + w_t + \theta w_{t-1}$ where $|\phi|<1$ and $w_t \sim \mathcal{N}(0, \sigma_w)$. 
The autocovariance function satisfies:
\[
	\gamma(h) -\phi \gamma(h-1)	= 0, h=2,3,\cdots
\]
And the general solution is:
\[
	\gamma(h) = c \phi^h, h=1,2,\cdots
\]
The initial conditions are 
\begin{align*}
	\gamma(0)	&=		\phi \gamma(1) + \sigma_w^2 [1 + \theta \phi + \theta^2] \\
	\gamma(1)	&=		\phi \gamma(0) + \sigma_w^2 \theta
\end{align*}
Solving for $\gamma(0)$ and $\gamma(1)$, we obtain: 
$$ \gamma(0) = \sigma_w^2 \frac{1 + 2 \theta \phi + \theta^2}{1 - \phi^2} \text{ and } \gamma(1) = \sigma_w^2 \frac{ (1 + \theta \phi) (\theta + \phi)}{1 - \phi^2}$$
Dividing by $\gamma(0)$ yields:
\[
	\rho(h) = \frac{ (1 + \theta \phi)  (\theta + \phi) } {  1 + 2 \theta \phi + \theta^2 } \phi^{h-1}, h \ge 1
\]

The squared exponential kernel with covariance function is defined as:
\[
	k(x_i, x_j) = \sigma^2 \exp{( \frac{- (x_1-x_2)^2 } {2 l^2 }	)}
\]
where
\begin{itemize}
	\item The length scale $l$ determines the length of the "wiggles". For $x_t$, using $\rho$ as $l$,  the GP process cannot extrapolate more than $\rho$ time steps away:
	as $x_{t1}$ and $x_{t2}$ are less correlated,  $|\rho|$  tends to zero, the exponential and $k(x_{t1}, x_{t2})$ tend to zero.
	
	\item The output variance $\sigma^2$ determines the average distance of your function away from its mean, we use  $\sigma^2 = \gamma(0)$.
\end{itemize}

The Matérn covariance function is the generalization of the squared exponential kernel and used 
to define the statistical covariance between measurements made at two points that are d units distant from each other:
\[
	C_\nu(d) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \big( \sqrt{2 \nu}	\frac{d}{\rho} \big)^\nu K_{\nu} \big(	\sqrt{2 \nu} \frac{d}{\rho}  \big)
\]
Then the process $x_t$ is the discrete time equivalents of Gaussian process models with Matérn covariance function with $\nu = \frac{1}{2}$ and $p=0$.
 The GP-equivalent of an ARIMA (1,2,1) model is: $ \mathcal{GP}( \beta_0 + \beta_1 t + \beta_2 t^2; \sigma^2 \exp{- (\frac{ |x_1-x_2| } {\rho}}) )$ where $\sigma^2 = \gamma(0)$ 
 and $\rho$ are given by the expressions above. 

\noindent \textbf{Problem 3. (15pt)} 
Derive the mean and covariance of  $\mathrm{P}(y|\theta)$ for the FITC approximation described in the lecture (this is obtained by marginalizing out $\mathbf{u}$ and $\mathbf{f})$.\\
%$$\p(\vect{y}|\theta) = \mathcal{N}\left( \mathbf{0},  \mathbf{K}_{fu} \mathbf{K}_{uu}^{-1}\mathbf{K}_{uf}  + \mathbf{D} + \sigma_y^2 \mathbf{I}\right).$$\\
\noindent \emph{Hint: one can think of the approximate model as a sequence of linear gaussian steps and use the usual simple gaussid.pdf properties.}\\

Given the two factor graphs in the slides of the lecture, $\{y_i\}$ are conditionally independent given  $\{f_i\}$  and depend directly on the $\{f_i\}$ which are conditionally independent  given $\{u_i\}$. 
The $\vect{u}$ variables summarize the dependencies in $\vect{f}$. Setting $p(\vect{u}) = \mathcal{N}(0,\matr{K_{uu}})$, 
the conditional distribution $p(f_t|\vect{u}) =  \mathcal{N}(f_t; K_{f_tu} K_{uu}^{-1} \vect{u}, K_{f_tf_t} -  K_{f_tu}  K_{uu}^{-1} K_{uf_t})$.

We have then:
\begin{align*}
	\vect{y_t}	&=	\vect{f_t} + \sigma_y \vect{\epsilon} \text{ with } \vect{\epsilon} \sim  \mathcal{N}(\vect{0},\vect{I}) \\
	\text{ let } D		&=	K_{f_tf_t} -  K_{f_tu}  K_{uu}^{-1} K_{uf_t} \\
	\vect{f_t}	&= 	 K_{f_tu} K_{uu}^{-1} \vect{u_t} + D \vect{\epsilon'} \text{ with } \vect{\epsilon'} \sim  \mathcal{N}(\vect{0},\vect{I}) \\
\end{align*}
$\vect{f_t}$ has 0 mean as it only depends on $\vect{u_t}$  which has 0 mean, the same for $\vect{y_t}$ which depends only on $\vect{f_t}$ with some noise.
Now:
\begin{align*}
	\Covar[\vect{y_t},\vect{y_s}]	&= \E_{\vect{u}, \vect{f}, \vect{\epsilon}}[(\vect{f_t} + \sigma_y \vect{\epsilon}) (\vect{f_s} + \sigma_y \vect{\epsilon})^T]	\\
							&= \E[\vect{f_t}, \vect{f_s}] + \sigma_y^2 \E[\vect{\epsilon}, \vect{\epsilon}] \\
							&= \E[\vect{f_t}, \vect{f_s}] +  \sigma_y^2 \vect{I} \\
	\E[\vect{f_t}, \vect{f_s}] 		&= \E[K_{f_tu} K_{uu}^{-1} \vect{u_t} \vect{u_s} K_{uu}^{-T}K_{uf_s}] + D \E[\vect{\epsilon'}, \vect{\epsilon'}] \\
							&= K_{f_tu} K_{uu}^{-1} \E[\vect{u_t}, \vect{u_s}]  K_{uu}^{-T}K_{uf_s} + D \vect{I} \\
							&=  K_{f_tu} K_{uu}^{-1} K_{uu}   K_{uu}^{-1} K_{uf_s} + D \\
							&= K_{f_tu} K_{uu}^{-1}   K_{uf_s} + D 
\end{align*}
Where the cross-terms in the second  and fourth equalities disappear as these terms are mutually independent.
Thus $$\p(\vect{y}|\theta) = \mathcal{N}\left( \mathbf{0},  \mathbf{K}_{fu} \mathbf{K}_{uu}^{-1}\mathbf{K}_{uf}  + \mathbf{D} + \sigma_y^2 \mathbf{I}\right).$$

\noindent \textbf{Problem 4. (10pt)} 
What GP-based model would you use for the Johnson\&Johnson quarterly earnings database?
Explain your choices. Would it matter if the goal of your analysis is to interpolate to account for missing data in the middle of the recorded time interval vs.\ extrapolating a decade into the future?\\

I used a linear combination of squared exponential kernel, linear, polynomial and periodic kernels. 
I tested the effect of the number of observations (sampling of the data) for the interpolation and forecasting (see ~\ref{problem41}).
Experimenting with different weighting of the kernels, I found that, a linear combination of  an SE and a periodic kernels captured the best the increasing trend, and some of the wiggles of the time series (see  ~\ref{problem42}).
The error bars showed that, when the observations were sparse the GP model confidence interval increased very rapidly confirming that,
 the GP model had larger confidence intervals for its predictions (see  ~\ref{problem42}: third and fourth points and forecasting beyond the last observation ~\ref{problem43}).
The same combination of kernels for the forecasting task, for different horizons, had a harder time to predict the wiggles of the process in the future (see  ~\ref{problem43}). 
Note the difference between interpolation in the middle when the GP uses neighboring points (see  ~\ref{problem42}: third and fourth points), vs. extrapolating in the future 
when the only available points are the last observations before the forecasting (see  ~\ref{problem43}). 
So for interpolating to account for missing data, the best choice could be an SE + a linear or polynomial model, which takes into account clusters of points and 
for forecasting an Exp-Sine-Squared  or a spectral mixture kernels might be more appropriate in an effort to gives weight to clusters of observations, but at the same time, to capture the dynamics of the process.
Lastly, in order to improve the overall model performance, I used a spectral mixture kernel which seemed to predict the swings of the data including the ones in the future (see  ~\ref{problem44}).

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/problem-4-1.png} 
	\caption{Johnson\&Johnson}
	\label{problem41}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/problem-4-2.png} 
	\caption{Interpolation using a weighted combinations of kernels}
	\label{problem42}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/problem-4-3.png} 
	\caption{SE + periodic kernels - Interpolation and Forecasting with error bars}
	\label{problem43}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{figures/problem-4-4.png} 
	\caption{Spectral Mixture Kernel with confidence intervals - two standard deviations}
	\label{problem44}
\end{figure}


\end{document}